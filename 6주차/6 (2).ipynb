{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IR Components\n",
    "# 1. Crawler + Indexer -> Crawler / Indexer\n",
    "# 2. Doc Analyzer -> (Improved) BoW\n",
    "# 3. Query -> 2번과 동일 -> DTM(TDM)\n",
    "# 4. Ranking Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-1. (Focused) Crawler (BFS) - 네이버 뉴스\n",
    "from requests import get\n",
    "from requests.compat import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "urls = ['https://news.naver.com']\n",
    "visited = list()\n",
    "\n",
    "path = './naver/'\n",
    "\n",
    "while urls:\n",
    "    seed = urls.pop(0) # Queue\n",
    "    visited.append(seed)\n",
    "    \n",
    "    dom = BeautifulSoup(get(seed).text, 'html.parser')\n",
    "    body = dom.select_one('#articleBodyContents')\n",
    "    \n",
    "    if body: # 뉴스\n",
    "        cid = re.search('rankingSectionId=(\\d+)', seed).group(1)\n",
    "        aid = re.search('aid=(\\d+)', seed).group(1)\n",
    "        \n",
    "        file = '{0}-{1}.txt'.format(cid, aid)\n",
    "        with open(path+file, 'w', encoding='utf-8') as f:\n",
    "            f.write(body.text)\n",
    "    else: # 링크\n",
    "        for a in dom.select('div[id^=ranking_] li > a'):\n",
    "            link = urljoin(seed, a['href'])\n",
    "            if link not in urls and link not in visited:\n",
    "                urls.append(link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from string import punctuation\n",
    "pattern1 = re.compile(r'[{}]'.format(re.escape(punctuation)))\n",
    "pattern2 = re.compile(r'\\b(\\w|[.])+@(?:[.]?\\w+)+\\b')\n",
    "pattern3 = re.compile(r'\\bhttps?://\\w+(?:[.]?\\w+)+\\b')\n",
    "pattern4 = re.compile(r'[^A-Za-z0-9가-힣ㄱ-ㅎㅏ-ㅣ ]')\n",
    "pattern5 = re.compile(r'\\b[a-z][A-Za-z0-9]+\\b')\n",
    "pattern6 = re.compile(r'\\s{2,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-2. Indexer\n",
    "from os import listdir\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "\n",
    "def fileids(path = './naver/'):\n",
    "    return [path+_ for _ in listdir(path)\n",
    "            if re.search('[.]txt$', _)]\n",
    "\n",
    "def cleaning(doc):\n",
    "    return pattern6.sub(' ',\n",
    "           pattern1.sub(' ',\n",
    "           pattern5.sub(' ',\n",
    "           pattern4.sub(' ',\n",
    "           pattern2.sub(' ', doc))))).strip()\n",
    "\n",
    "def tokenizer1(doc): # 어절\n",
    "    return doc.split()\n",
    "\n",
    "def tokenizer2(tokens, n=2): # 어절 Ngram\n",
    "    ngram = list()\n",
    "    for i in range(len(tokens) - (n-1)):\n",
    "        ngram.append( )\n",
    "    return ngram\n",
    "\n",
    "def tokenizer3(doc, n=2): # 음절 Ngram\n",
    "    ngram = list()\n",
    "    for i in range(len(doc) - (n-1)):\n",
    "        ngram.append(doc[i:i+n])\n",
    "    return ngram\n",
    "\n",
    "def tokenizer4(doc): # 형태소\n",
    "    return [_ for _ in okt.morphs(doc) if 1 < len(_) < 8]\n",
    "\n",
    "def tokenizer5(doc): # 명사\n",
    "    return [_ for _ in okt.nouns(doc) if 1 < len(_) < 8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(file):\n",
    "    print(file)\n",
    "    terms = defaultdict(lambda:0)\n",
    "    \n",
    "    with open(file, 'r') as f:\n",
    "        news = cleaning(f.read())\n",
    "\n",
    "    for _ in tokenizer1(news):\n",
    "        terms[_] += 1\n",
    "    for _ in tokenizer2(list(terms.keys())):\n",
    "        terms[_] += 1\n",
    "    for _ in tokenizer3(news):\n",
    "        terms[_] += 1\n",
    "    for _ in tokenizer4(news):\n",
    "        terms[_] += 1\n",
    "        \n",
    "    return terms\n",
    "\n",
    "def indexer(file):\n",
    "    lexicon = dict()\n",
    "    \n",
    "    for k, v in get_tokens(file).items():\n",
    "        if k not in lexicon:\n",
    "            lexicon[k] = 1\n",
    "                \n",
    "    return lexicon\n",
    "\n",
    "def mergeLexicon(Lexicon, i, LocalLexicon, invertedIndex):\n",
    "    for k, v in LocalLexicon.items():\n",
    "        if k in Lexicon.keys():\n",
    "            termInfo = (i, Lexicon[k])\n",
    "            invertedIndex.append(termInfo)\n",
    "            Lexicon[k] = invertedIndex.index(termInfo)\n",
    "        else:\n",
    "            termInfo = (i, -1)\n",
    "            invertedIndex.append(termInfo)\n",
    "            Lexicon[k] = invertedIndex.index(termInfo)\n",
    "            \n",
    "    return Lexicon, invertedIndex\n",
    "\n",
    "def sortedLexicon(Lexicon, invertedIndex):\n",
    "    sortedIndex = list()\n",
    "    \n",
    "    for k, v in Lexicon.items():\n",
    "        pos1 = v\n",
    "        pos2 = len(sortedIndex)\n",
    "        df = 0\n",
    "        \n",
    "        while pos1 > -1:\n",
    "            termInfo = invertedIndex[pos1]\n",
    "            pos1 = termInfo[1]\n",
    "            df += 1\n",
    "            sortedIndex.append(termInfo[0])\n",
    "            \n",
    "        Lexicon[k] = (df, pos2)\n",
    "        \n",
    "    return Lexicon, sortedIndex\n",
    "            \n",
    "# Liked List(동적 리스트)\n",
    "# termInfo = (문서번호, 다음위치)\n",
    "# Lexicon[k] -> (위치)(문서번호, 다음위치) -> (위치)(문서번호, 다음위치:-1)\n",
    "\n",
    "# [37:39]\n",
    "# Lexicon : 'ABC' -> ? (line:36)\n",
    "#                    0 (line:39)\n",
    "\n",
    "#                    (i, -1) (line:37)\n",
    "# invertedIndex : 0:(i, -1) (line:38)\n",
    "# [32:35]\n",
    "#                    (i, Lexicon['ABC']:0) (line:33)\n",
    "# invertedIndex : 0:(i, -1)\n",
    "#                 1:(i,  0) (line:34)\n",
    "# Lexicon : 'ABC' -> 1 (line:35)\n",
    "\n",
    "# Lexicon, invertedIndex, sortedIndex\n",
    "# Lexicon[k]                  invertedIndex           sortedIndex\n",
    "# \"ABC\" -> 190031           190031:(60, 50)               60\n",
    "#                               50:(2, -1)                 2\n",
    "#       -> (2, 0)\n",
    "#          df, pos [0:df+1]\n",
    "#                   60, 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./naver/102-0003023007.txt\n",
      "./naver/100-0003551166.txt\n",
      "./naver/101-0004729825.txt\n",
      "./naver/104-0002507337.txt\n",
      "./naver/104-0003023003.txt\n",
      "./naver/101-0003022999.txt\n",
      "./naver/100-0004391547.txt\n",
      "./naver/105-0001707186.txt\n",
      "./naver/105-0002195462.txt\n",
      "./naver/103-0003023010.txt\n",
      "./naver/105-0000564412.txt\n",
      "./naver/101-0003551211.txt\n",
      "./naver/105-0001707231.txt\n",
      "./naver/103-0001472759.txt\n",
      "./naver/104-0003551212.txt\n",
      "./naver/104-0003551206.txt\n",
      "./naver/103-0000832054.txt\n",
      "./naver/100-0011788427.txt\n",
      "./naver/102-0011788086.txt\n",
      "./naver/104-0000890782.txt\n",
      "./naver/103-0004627628.txt\n",
      "./naver/100-0004391479.txt\n"
     ]
    }
   ],
   "source": [
    "Collection = fileids()\n",
    "Lexicon = dict()\n",
    "invertedIndex = list()\n",
    "\n",
    "for i, doc in enumerate(Collection): # 전체 문서집합\n",
    "    localLexicon = indexer(doc) # 문서 당 처리 -> 단어:빈도{0,1}\n",
    "    Lexicon, invertedIndex = mergeLexicon(Lexicon,\n",
    "                i, localLexicon, invertedIndex) # 문서 합치고(역색인구조)\n",
    "\n",
    "Lexicon, invertedIndex = sortedLexicon(Lexicon, invertedIndex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in list(Lexicon.keys())[:10]:\n",
    "    termInfo = Lexicon[k]\n",
    "    docList = invertedIndex[termInfo[1]:sum(termInfo)]\n",
    "    print(k, termInfo, docList, len(docList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "voca, posting = indexer(fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = '대형 초밥 체인점 한국어 메뉴에만 냉수 180엔 부과재일 교포 A씨 트위터에 해당 사건 공유해 일파만파'\n",
    "qterms = tokenizer1(query)\n",
    "qterms += tokenizer2(qterms)\n",
    "qterms += tokenizer3(query)\n",
    "qterms += tokenizer4(query)\n",
    "qterms += tokenizer5(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = list()\n",
    "for t in qterms:\n",
    "    temp = list()\n",
    "    if t in voca:\n",
    "        print(t)\n",
    "        pos = voca[t]\n",
    "        while pos > -1:\n",
    "            info = posting[pos]\n",
    "            pos = info[-1]\n",
    "            temp.append(str(info[1]))\n",
    "            result.append(info[1])\n",
    "    print(','.join(temp))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(set(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
